{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "bmyoEkQLF24G",
    "outputId": "62e3407b-5ce0-4051-cfa9-3e2e9590106d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import cifar10\n",
    "from resnet_builder import resnet \n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "VR-Nbj96F43S",
    "outputId": "658d7a7f-4646-4da7-bf22-8e0e557e5640"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "x_train = x_train / 255.\n",
    "x_test = x_test / 255.\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "z7iy9yVIF6q4",
    "outputId": "cac4e283-8d1d-4d43-9f0f-36bfcbb8822b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0817 15:13:39.171892 139752126551936 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0817 15:13:39.190446 139752126551936 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0817 15:13:39.194158 139752126551936 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4185: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
      "\n",
      "W0817 15:13:39.224147 139752126551936 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0817 15:13:39.224937 139752126551936 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "W0817 15:13:39.849315 139752126551936 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "W0817 15:13:42.560348 139752126551936 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3980: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 32, 32, 16)   448         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 32, 32, 16)   64          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 32, 32, 16)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 32, 32, 16)   272         activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 32, 32, 16)   64          conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 32, 32, 16)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 32, 32, 16)   2320        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 32, 32, 16)   64          conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 32, 32, 16)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 32, 32, 64)   1088        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 32, 32, 64)   1088        activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 32, 32, 64)   0           conv2d_5[0][0]                   \n",
      "                                                                 conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 32, 32, 64)   256         add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 32, 32, 64)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 32, 32, 16)   1040        activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32, 32, 16)   64          conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 32, 32, 16)   0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 32, 32, 16)   2320        activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 32, 32, 16)   64          conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 32, 32, 16)   0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 32, 32, 64)   1088        activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 32, 32, 64)   0           add_1[0][0]                      \n",
      "                                                                 conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 32, 32, 64)   256         add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 32, 32, 64)   0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 32, 32, 16)   1040        activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 32, 32, 16)   64          conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 32, 32, 16)   0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 32, 32, 16)   2320        activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 32, 32, 16)   64          conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 32, 32, 16)   0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 32, 32, 64)   1088        activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 32, 32, 64)   0           add_2[0][0]                      \n",
      "                                                                 conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 32, 32, 64)   256         add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 32, 32, 64)   0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 16, 16, 64)   4160        activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 16, 16, 64)   256         conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 16, 16, 64)   0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 16, 16, 64)   36928       activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 16, 16, 64)   256         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 16, 16, 64)   0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 16, 16, 128)  8320        add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 16, 16, 128)  8320        activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 16, 16, 128)  0           conv2d_15[0][0]                  \n",
      "                                                                 conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 16, 16, 128)  512         add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 16, 16, 128)  0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 16, 16, 64)   8256        activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 16, 16, 64)   256         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 16, 16, 64)   0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 16, 16, 64)   36928       activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 16, 16, 64)   256         conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 16, 16, 64)   0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 16, 16, 128)  8320        activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 16, 16, 128)  0           add_4[0][0]                      \n",
      "                                                                 conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 16, 16, 128)  512         add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 16, 16, 128)  0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 16, 16, 64)   8256        activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 16, 16, 64)   256         conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 16, 16, 64)   0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 16, 16, 64)   36928       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 16, 16, 64)   256         conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 16, 16, 64)   0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 16, 16, 128)  8320        activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 16, 16, 128)  0           add_5[0][0]                      \n",
      "                                                                 conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 16, 16, 128)  512         add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 16, 16, 128)  0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 8, 8, 128)    16512       activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 8, 8, 128)    512         conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 8, 8, 128)    0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 8, 8, 128)    147584      activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 8, 8, 128)    512         conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 8, 8, 128)    0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 8, 8, 256)    33024       add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 8, 8, 256)    33024       activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 8, 8, 256)    0           conv2d_25[0][0]                  \n",
      "                                                                 conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 8, 8, 256)    1024        add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 8, 8, 256)    0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 8, 8, 128)    32896       activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 8, 8, 128)    512         conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 8, 8, 128)    0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 8, 8, 128)    147584      activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 8, 8, 128)    512         conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 8, 8, 128)    0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 8, 8, 256)    33024       activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 8, 8, 256)    0           add_7[0][0]                      \n",
      "                                                                 conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 8, 8, 256)    1024        add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 8, 8, 256)    0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 8, 8, 128)    32896       activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 8, 8, 128)    512         conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 8, 8, 128)    0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 8, 8, 128)    147584      activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 8, 8, 128)    512         conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 8, 8, 128)    0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 8, 8, 256)    33024       activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 8, 8, 256)    0           add_8[0][0]                      \n",
      "                                                                 conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 8, 8, 256)    1024        add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 8, 8, 256)    0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 1, 1, 256)    0           activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 256)          0           average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 10)           2570        flatten_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 849,002\n",
      "Trainable params: 843,786\n",
      "Non-trainable params: 5,216\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = resnet(input_shape=(32,32,3)) \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "NkQ_dzAOGyo6",
    "outputId": "dd0870e0-dc10-4484-f0eb-4a423a34e2af"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0817 15:13:42.660781 139752126551936 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0817 15:13:43.165681 139752126551936 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "50000/50000 [==============================] - 42s 840us/step - loss: 1.8974 - acc: 0.4986 - val_loss: 1.9310 - val_acc: 0.5060\n",
      "Epoch 2/100\n",
      "50000/50000 [==============================] - 34s 677us/step - loss: 1.4008 - acc: 0.6497 - val_loss: 1.5727 - val_acc: 0.5740\n",
      "Epoch 3/100\n",
      "50000/50000 [==============================] - 34s 675us/step - loss: 1.1753 - acc: 0.7161 - val_loss: 1.5504 - val_acc: 0.5966\n",
      "Epoch 4/100\n",
      "50000/50000 [==============================] - 34s 676us/step - loss: 1.0235 - acc: 0.7635 - val_loss: 1.5079 - val_acc: 0.6467\n",
      "Epoch 5/100\n",
      "50000/50000 [==============================] - 34s 674us/step - loss: 0.9191 - acc: 0.7964 - val_loss: 1.4800 - val_acc: 0.6312\n",
      "Epoch 6/100\n",
      "50000/50000 [==============================] - 34s 674us/step - loss: 0.8258 - acc: 0.8302 - val_loss: 1.3369 - val_acc: 0.6807\n",
      "Epoch 7/100\n",
      "50000/50000 [==============================] - 34s 676us/step - loss: 0.7547 - acc: 0.8528 - val_loss: 1.3398 - val_acc: 0.6795\n",
      "Epoch 8/100\n",
      "50000/50000 [==============================] - 34s 674us/step - loss: 0.6893 - acc: 0.8752 - val_loss: 1.5398 - val_acc: 0.6539\n",
      "Epoch 9/100\n",
      "50000/50000 [==============================] - 34s 678us/step - loss: 0.6403 - acc: 0.8921 - val_loss: 1.3623 - val_acc: 0.6964\n",
      "Epoch 10/100\n",
      "50000/50000 [==============================] - 34s 677us/step - loss: 0.6063 - acc: 0.9027 - val_loss: 1.5893 - val_acc: 0.6702\n",
      "Epoch 11/100\n",
      "50000/50000 [==============================] - 34s 679us/step - loss: 0.5695 - acc: 0.9168 - val_loss: 1.2994 - val_acc: 0.7306\n",
      "Epoch 12/100\n",
      "50000/50000 [==============================] - 34s 678us/step - loss: 0.5475 - acc: 0.9245 - val_loss: 1.4068 - val_acc: 0.7167\n",
      "Epoch 13/100\n",
      "50000/50000 [==============================] - 34s 680us/step - loss: 0.5189 - acc: 0.9357 - val_loss: 1.2530 - val_acc: 0.7483\n",
      "Epoch 14/100\n",
      "50000/50000 [==============================] - 34s 680us/step - loss: 0.5077 - acc: 0.9395 - val_loss: 1.3566 - val_acc: 0.7275\n",
      "Epoch 15/100\n",
      "50000/50000 [==============================] - 34s 678us/step - loss: 0.5002 - acc: 0.9404 - val_loss: 1.4849 - val_acc: 0.7135\n",
      "Epoch 16/100\n",
      "50000/50000 [==============================] - 34s 680us/step - loss: 0.4868 - acc: 0.9460 - val_loss: 1.6576 - val_acc: 0.6913\n",
      "Epoch 17/100\n",
      "50000/50000 [==============================] - 34s 677us/step - loss: 0.4782 - acc: 0.9482 - val_loss: 1.5599 - val_acc: 0.6912\n",
      "Epoch 18/100\n",
      "50000/50000 [==============================] - 34s 677us/step - loss: 0.4718 - acc: 0.9503 - val_loss: 1.5848 - val_acc: 0.6969\n",
      "Epoch 19/100\n",
      "50000/50000 [==============================] - 34s 677us/step - loss: 0.4725 - acc: 0.9488 - val_loss: 1.6593 - val_acc: 0.7071\n",
      "Epoch 20/100\n",
      "50000/50000 [==============================] - 34s 676us/step - loss: 0.4650 - acc: 0.9526 - val_loss: 1.5826 - val_acc: 0.7073\n",
      "Epoch 21/100\n",
      "50000/50000 [==============================] - 34s 676us/step - loss: 0.4611 - acc: 0.9540 - val_loss: 1.7575 - val_acc: 0.6860\n",
      "Epoch 22/100\n",
      "50000/50000 [==============================] - 34s 677us/step - loss: 0.4557 - acc: 0.9562 - val_loss: 1.4998 - val_acc: 0.7166\n",
      "Epoch 23/100\n",
      "50000/50000 [==============================] - 34s 678us/step - loss: 0.4456 - acc: 0.9578 - val_loss: 1.7103 - val_acc: 0.6949\n",
      "Epoch 24/100\n",
      "50000/50000 [==============================] - 34s 678us/step - loss: 0.4474 - acc: 0.9568 - val_loss: 1.6035 - val_acc: 0.7163\n",
      "Epoch 25/100\n",
      "50000/50000 [==============================] - 34s 677us/step - loss: 0.4416 - acc: 0.9592 - val_loss: 1.5404 - val_acc: 0.7130\n",
      "Epoch 26/100\n",
      "50000/50000 [==============================] - 34s 677us/step - loss: 0.4533 - acc: 0.9553 - val_loss: 2.2707 - val_acc: 0.6191\n",
      "Epoch 27/100\n",
      "50000/50000 [==============================] - 34s 678us/step - loss: 0.4406 - acc: 0.9591 - val_loss: 1.7306 - val_acc: 0.6997\n",
      "Epoch 28/100\n",
      "50000/50000 [==============================] - 34s 677us/step - loss: 0.4324 - acc: 0.9615 - val_loss: 1.8812 - val_acc: 0.6719\n",
      "Epoch 29/100\n",
      "50000/50000 [==============================] - 34s 676us/step - loss: 0.4353 - acc: 0.9605 - val_loss: 1.8565 - val_acc: 0.7052\n",
      "Epoch 30/100\n",
      "50000/50000 [==============================] - 34s 675us/step - loss: 0.4361 - acc: 0.9599 - val_loss: 1.5895 - val_acc: 0.7237\n",
      "Epoch 31/100\n",
      "50000/50000 [==============================] - 34s 676us/step - loss: 0.4353 - acc: 0.9596 - val_loss: 1.4430 - val_acc: 0.7493\n",
      "Epoch 32/100\n",
      "50000/50000 [==============================] - 34s 677us/step - loss: 0.4265 - acc: 0.9625 - val_loss: 1.7363 - val_acc: 0.6988\n",
      "Epoch 33/100\n",
      "50000/50000 [==============================] - 34s 677us/step - loss: 0.4228 - acc: 0.9639 - val_loss: 1.8011 - val_acc: 0.7165\n",
      "Epoch 34/100\n",
      "50000/50000 [==============================] - 34s 677us/step - loss: 0.4213 - acc: 0.9646 - val_loss: 1.6156 - val_acc: 0.7226\n",
      "Epoch 35/100\n",
      "50000/50000 [==============================] - 34s 678us/step - loss: 0.4235 - acc: 0.9622 - val_loss: 1.6466 - val_acc: 0.7200\n",
      "Epoch 36/100\n",
      "50000/50000 [==============================] - 34s 678us/step - loss: 0.4217 - acc: 0.9626 - val_loss: 1.8100 - val_acc: 0.6963\n",
      "Epoch 37/100\n",
      "50000/50000 [==============================] - 34s 679us/step - loss: 0.4207 - acc: 0.9633 - val_loss: 1.4724 - val_acc: 0.7418\n",
      "Epoch 38/100\n",
      "50000/50000 [==============================] - 34s 677us/step - loss: 0.4084 - acc: 0.9681 - val_loss: 3.6622 - val_acc: 0.5344\n",
      "Epoch 39/100\n",
      "50000/50000 [==============================] - 34s 679us/step - loss: 0.4159 - acc: 0.9639 - val_loss: 1.4735 - val_acc: 0.7168\n",
      "Epoch 40/100\n",
      "50000/50000 [==============================] - 34s 676us/step - loss: 0.4190 - acc: 0.9622 - val_loss: 1.5609 - val_acc: 0.7218\n",
      "Epoch 41/100\n",
      "50000/50000 [==============================] - 34s 676us/step - loss: 0.4043 - acc: 0.9679 - val_loss: 2.1769 - val_acc: 0.6672\n",
      "Epoch 42/100\n",
      "50000/50000 [==============================] - 34s 676us/step - loss: 0.4143 - acc: 0.9633 - val_loss: 1.8280 - val_acc: 0.7030\n",
      "Epoch 43/100\n",
      "50000/50000 [==============================] - 34s 677us/step - loss: 0.4021 - acc: 0.9673 - val_loss: 1.4516 - val_acc: 0.7479\n",
      "Epoch 44/100\n",
      "50000/50000 [==============================] - 34s 679us/step - loss: 0.4062 - acc: 0.9653 - val_loss: 1.9487 - val_acc: 0.7038\n",
      "Epoch 45/100\n",
      "50000/50000 [==============================] - 34s 678us/step - loss: 0.4063 - acc: 0.9662 - val_loss: 2.3455 - val_acc: 0.6477\n",
      "Epoch 46/100\n",
      "50000/50000 [==============================] - 34s 680us/step - loss: 0.4044 - acc: 0.9651 - val_loss: 1.4547 - val_acc: 0.7408\n",
      "Epoch 47/100\n",
      "50000/50000 [==============================] - 34s 677us/step - loss: 0.3976 - acc: 0.9671 - val_loss: 1.7589 - val_acc: 0.7263\n",
      "Epoch 48/100\n",
      "50000/50000 [==============================] - 34s 676us/step - loss: 0.3968 - acc: 0.9675 - val_loss: 1.7064 - val_acc: 0.6862\n",
      "Epoch 49/100\n",
      "50000/50000 [==============================] - 34s 677us/step - loss: 0.4027 - acc: 0.9651 - val_loss: 1.9749 - val_acc: 0.6806\n",
      "Epoch 50/100\n",
      "50000/50000 [==============================] - 34s 676us/step - loss: 0.3947 - acc: 0.9681 - val_loss: 1.5949 - val_acc: 0.7223\n",
      "Epoch 51/100\n",
      "50000/50000 [==============================] - 34s 677us/step - loss: 0.3906 - acc: 0.9691 - val_loss: 2.1467 - val_acc: 0.6634\n",
      "Epoch 52/100\n",
      "50000/50000 [==============================] - 34s 676us/step - loss: 0.3937 - acc: 0.9670 - val_loss: 1.8872 - val_acc: 0.6903\n",
      "Epoch 53/100\n",
      "50000/50000 [==============================] - 34s 677us/step - loss: 0.3939 - acc: 0.9670 - val_loss: 1.4975 - val_acc: 0.7595\n",
      "Epoch 54/100\n",
      "50000/50000 [==============================] - 34s 679us/step - loss: 0.3885 - acc: 0.9683 - val_loss: 1.5563 - val_acc: 0.7189\n",
      "Epoch 55/100\n",
      "50000/50000 [==============================] - 34s 679us/step - loss: 0.3900 - acc: 0.9663 - val_loss: 1.4660 - val_acc: 0.7336\n",
      "Epoch 56/100\n",
      "50000/50000 [==============================] - 34s 679us/step - loss: 0.3873 - acc: 0.9689 - val_loss: 1.8267 - val_acc: 0.7382\n",
      "Epoch 57/100\n",
      "50000/50000 [==============================] - 34s 677us/step - loss: 0.3868 - acc: 0.9683 - val_loss: 1.4179 - val_acc: 0.7598\n",
      "Epoch 58/100\n",
      "50000/50000 [==============================] - 34s 678us/step - loss: 0.3790 - acc: 0.9704 - val_loss: 2.9113 - val_acc: 0.6076\n",
      "Epoch 59/100\n",
      "50000/50000 [==============================] - 34s 677us/step - loss: 0.3864 - acc: 0.9678 - val_loss: 1.5114 - val_acc: 0.7480\n",
      "Epoch 60/100\n",
      "50000/50000 [==============================] - 34s 677us/step - loss: 0.3789 - acc: 0.9707 - val_loss: 2.1353 - val_acc: 0.7009\n",
      "Epoch 61/100\n",
      "50000/50000 [==============================] - 34s 675us/step - loss: 0.3812 - acc: 0.9692 - val_loss: 1.6094 - val_acc: 0.7399\n",
      "Epoch 62/100\n",
      "50000/50000 [==============================] - 34s 674us/step - loss: 0.3746 - acc: 0.9708 - val_loss: 1.8172 - val_acc: 0.6983\n",
      "Epoch 63/100\n",
      "50000/50000 [==============================] - 34s 677us/step - loss: 0.3790 - acc: 0.9700 - val_loss: 1.3102 - val_acc: 0.7751\n",
      "Epoch 64/100\n",
      "50000/50000 [==============================] - 34s 678us/step - loss: 0.3868 - acc: 0.9662 - val_loss: 2.1645 - val_acc: 0.7036\n",
      "Epoch 65/100\n",
      "50000/50000 [==============================] - 34s 678us/step - loss: 0.3792 - acc: 0.9692 - val_loss: 1.6541 - val_acc: 0.7393\n",
      "Epoch 66/100\n",
      "50000/50000 [==============================] - 34s 679us/step - loss: 0.3708 - acc: 0.9725 - val_loss: 1.5455 - val_acc: 0.7378\n",
      "Epoch 67/100\n",
      "50000/50000 [==============================] - 34s 679us/step - loss: 0.3722 - acc: 0.9707 - val_loss: 1.5725 - val_acc: 0.7460\n",
      "Epoch 68/100\n",
      "50000/50000 [==============================] - 34s 677us/step - loss: 0.3714 - acc: 0.9703 - val_loss: 1.4903 - val_acc: 0.7409\n",
      "Epoch 69/100\n",
      "50000/50000 [==============================] - 34s 679us/step - loss: 0.3708 - acc: 0.9705 - val_loss: 1.7289 - val_acc: 0.7292\n",
      "Epoch 70/100\n",
      "50000/50000 [==============================] - 34s 680us/step - loss: 0.3761 - acc: 0.9683 - val_loss: 1.5379 - val_acc: 0.7222\n",
      "Epoch 71/100\n",
      "50000/50000 [==============================] - 34s 673us/step - loss: 0.3665 - acc: 0.9723 - val_loss: 1.6150 - val_acc: 0.7359\n",
      "Epoch 72/100\n",
      "50000/50000 [==============================] - 34s 674us/step - loss: 0.3686 - acc: 0.9703 - val_loss: 1.7220 - val_acc: 0.7384\n",
      "Epoch 73/100\n",
      "50000/50000 [==============================] - 34s 679us/step - loss: 0.3704 - acc: 0.9706 - val_loss: 2.2984 - val_acc: 0.6505\n",
      "Epoch 74/100\n",
      "50000/50000 [==============================] - 34s 679us/step - loss: 0.3612 - acc: 0.9728 - val_loss: 1.6888 - val_acc: 0.7358\n",
      "Epoch 75/100\n",
      "50000/50000 [==============================] - 34s 680us/step - loss: 0.3686 - acc: 0.9707 - val_loss: 1.6305 - val_acc: 0.7322\n",
      "Epoch 76/100\n",
      "50000/50000 [==============================] - 34s 677us/step - loss: 0.3744 - acc: 0.9682 - val_loss: 1.3707 - val_acc: 0.7631\n",
      "Epoch 77/100\n",
      "50000/50000 [==============================] - 34s 675us/step - loss: 0.3557 - acc: 0.9744 - val_loss: 1.6379 - val_acc: 0.7316\n",
      "Epoch 78/100\n",
      "50000/50000 [==============================] - 34s 674us/step - loss: 0.3640 - acc: 0.9707 - val_loss: 1.6139 - val_acc: 0.7312\n",
      "Epoch 79/100\n",
      "50000/50000 [==============================] - 34s 676us/step - loss: 0.3603 - acc: 0.9721 - val_loss: 1.5166 - val_acc: 0.7356\n",
      "Epoch 80/100\n",
      "50000/50000 [==============================] - 34s 675us/step - loss: 0.3574 - acc: 0.9723 - val_loss: 1.5921 - val_acc: 0.7525\n",
      "Epoch 81/100\n",
      "50000/50000 [==============================] - 34s 676us/step - loss: 0.3627 - acc: 0.9711 - val_loss: 1.7656 - val_acc: 0.7359\n",
      "Epoch 82/100\n",
      "50000/50000 [==============================] - 34s 677us/step - loss: 0.3612 - acc: 0.9713 - val_loss: 1.4910 - val_acc: 0.7728\n",
      "Epoch 83/100\n",
      "50000/50000 [==============================] - 34s 676us/step - loss: 0.3576 - acc: 0.9718 - val_loss: 1.6606 - val_acc: 0.7443\n",
      "Epoch 84/100\n",
      "50000/50000 [==============================] - 34s 675us/step - loss: 0.3592 - acc: 0.9714 - val_loss: 1.6112 - val_acc: 0.7366\n",
      "Epoch 85/100\n",
      "50000/50000 [==============================] - 34s 676us/step - loss: 0.3571 - acc: 0.9708 - val_loss: 1.3637 - val_acc: 0.7646\n",
      "Epoch 86/100\n",
      "50000/50000 [==============================] - 34s 675us/step - loss: 0.3557 - acc: 0.9729 - val_loss: 1.7154 - val_acc: 0.6914\n",
      "Epoch 87/100\n",
      "50000/50000 [==============================] - 34s 674us/step - loss: 0.3536 - acc: 0.9728 - val_loss: 1.4830 - val_acc: 0.7538\n",
      "Epoch 88/100\n",
      "50000/50000 [==============================] - 34s 674us/step - loss: 0.3524 - acc: 0.9728 - val_loss: 1.5484 - val_acc: 0.7609\n",
      "Epoch 89/100\n",
      "50000/50000 [==============================] - 34s 674us/step - loss: 0.3557 - acc: 0.9717 - val_loss: 2.1854 - val_acc: 0.6820\n",
      "Epoch 90/100\n",
      "50000/50000 [==============================] - 34s 677us/step - loss: 0.3580 - acc: 0.9705 - val_loss: 1.5395 - val_acc: 0.7306\n",
      "Epoch 91/100\n",
      "50000/50000 [==============================] - 34s 673us/step - loss: 0.3526 - acc: 0.9731 - val_loss: 1.3463 - val_acc: 0.7644\n",
      "Epoch 92/100\n",
      "50000/50000 [==============================] - 34s 677us/step - loss: 0.3393 - acc: 0.9772 - val_loss: 1.4573 - val_acc: 0.7592\n",
      "Epoch 93/100\n",
      "50000/50000 [==============================] - 34s 675us/step - loss: 0.3484 - acc: 0.9727 - val_loss: 1.5014 - val_acc: 0.7497\n",
      "Epoch 94/100\n",
      "50000/50000 [==============================] - 34s 675us/step - loss: 0.3537 - acc: 0.9715 - val_loss: 1.5412 - val_acc: 0.7408\n",
      "Epoch 95/100\n",
      "50000/50000 [==============================] - 34s 676us/step - loss: 0.3530 - acc: 0.9712 - val_loss: 1.6465 - val_acc: 0.7128\n",
      "Epoch 96/100\n",
      "50000/50000 [==============================] - 34s 677us/step - loss: 0.3431 - acc: 0.9751 - val_loss: 1.5296 - val_acc: 0.7573\n",
      "Epoch 97/100\n",
      "50000/50000 [==============================] - 34s 677us/step - loss: 0.3469 - acc: 0.9736 - val_loss: 1.2825 - val_acc: 0.7798\n",
      "Epoch 98/100\n",
      "50000/50000 [==============================] - 34s 676us/step - loss: 0.3426 - acc: 0.9739 - val_loss: 1.8691 - val_acc: 0.7311\n",
      "Epoch 99/100\n",
      "50000/50000 [==============================] - 34s 677us/step - loss: 0.3482 - acc: 0.9720 - val_loss: 2.0829 - val_acc: 0.7250\n",
      "Epoch 100/100\n",
      "50000/50000 [==============================] - 34s 675us/step - loss: 0.3469 - acc: 0.9722 - val_loss: 1.6159 - val_acc: 0.7399\n",
      "Test loss: 1.6159161293029785\n",
      "Test accuracy: 0.7399\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128 # batch 的大小，如果出現 OOM error，請降低這個值\n",
    "num_classes = 10 # 類別的數量，Cifar 10 共有 10 個類別\n",
    "epochs = 100 # 訓練整個資料集共 100個循環\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "DAY100.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
